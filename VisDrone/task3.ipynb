{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 3: single-object tracking challenge. The task aims to estimate the state of a target, indicated in the first frame, in the subsequent video frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import imageio.v2 as io\n",
    "import cv2\n",
    "import torch\n",
    "import os\n",
    "import sys\n",
    "from utils.utils_funcitons import *\n",
    "device= \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "from ultralytics import YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = r'.\\dataset\\Task3_Single_Object_Tracking'\n",
    "sequences_folder = os.path.join(dataset_path, 'sequences')\n",
    "annotations_folder = os.path.join(dataset_path, 'annotations')\n",
    "model_path = r'.\\models\\best_yolov8n.pt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOLOv8 Object Tracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLO(model_path)\n",
    "\n",
    "# Images folder path\n",
    "#img_folder = os.path.join(sequences_folder, 'uav0000024_00000_s') #people\n",
    "#img_folder = os.path.join(sequences_folder, 'uav0000029_01102_s') #motor\n",
    "img_folder = os.path.join(sequences_folder, 'uav0000086_00870_s') #people\n",
    "\n",
    "#img_folder = os.path.join(dataset_path, r'sequences\\uav0000120_04775_v')\n",
    "#img_folder = os.path.join(dataset_path, r'sequences\\uav0000370_00001_v')\n",
    "img_names = sorted(os.listdir(img_folder))\n",
    "\n",
    "# Video size\n",
    "width, height = 1280, 720\n",
    "\n",
    "# Codec and VideoWriter\n",
    "codec = cv2.VideoWriter_fourcc(*'XVID')\n",
    "fps = 30\n",
    "output_video = cv2.VideoWriter(r'.\\outputs\\Task3\\output_yolo.mp4', codec, fps, (width, height))\n",
    "\n",
    "# Frame processing\n",
    "\n",
    "for i, img in enumerate(img_names):\n",
    "    img_fullpath = os.path.join(img_folder, img)\n",
    "    frame = cv2.imread(img_fullpath)\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    if frame is not None:\n",
    "        # Redimensionar la imagen si es necesario\n",
    "        if width and height:\n",
    "            result_boxes = track_objects(model, frame, objects=['pedestrian'])\n",
    "            # Detection control. If no detections were found, use the original frame\n",
    "            if len(result_boxes[0].cls) > 0 and result_boxes[0].id != None: \n",
    "                pred_cls = result_boxes[0].cls.detach().cpu().numpy()\n",
    "                conf = result_boxes[0].conf.detach().cpu().numpy()\n",
    "                bboxes_xyxy = result_boxes[0].xyxy.detach().cpu().numpy()\n",
    "                track_id = result_boxes[0].id\n",
    "\n",
    "                obj_id_to_track = 44\n",
    "                pos = torch.nonzero(track_id == obj_id_to_track)[0,0] if torch.nonzero(track_id == obj_id_to_track).nelement() != 0 else None\n",
    "\n",
    "                if pos != None:\n",
    "                    cv2.rectangle(frame, (int(bboxes_xyxy[pos][0]), int(bboxes_xyxy[pos][1])), (int(bboxes_xyxy[pos][2]), int(bboxes_xyxy[pos][3])), (0,0,255), 2)\n",
    "                    cv2.putText(frame, f\"People-{obj_id_to_track}\", (int(bboxes_xyxy[pos][0]) + 10, int(bboxes_xyxy[pos][1]) - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0,0,255), 1)        \n",
    "\n",
    "                frame_bgr = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
    "                frame_bgr = cv2.resize(frame_bgr, (width, height))\n",
    "            else:\n",
    "                frame = cv2.resize(frame, (width, height))\n",
    "                frame_bgr = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "                '''\n",
    "                #SOLO TRACKEAMOS EL PRIMER OBJETO ENCONTRADO\n",
    "                if i == 0:\n",
    "                    obj_id_to_track = 3\n",
    "                    pos = torch.nonzero(track_id == obj_id_to_track)[0,0] if torch.nonzero(track_id == obj_id_to_track).nelement() != 0 else 0\n",
    "\n",
    "                    #first_id = track_id.detach().cpu().numpy()[0]\n",
    "                    cv2.rectangle(frame, (int(bboxes_xyxy[0]), int(bboxes_xyxy[1])), (int(bboxes_xyxy[2]), int(bboxes_xyxy[3])), (0,0,255), 2)\n",
    "                    cv2.putText(frame, f\"Pedestrian-{first_id}\", (int(bboxes_xyxy[0]) + 10, int(bboxes_xyxy[1]) - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0,0,255), 1)\n",
    "                    display_image(frame)\n",
    "                elif torch.any(torch.eq(track_id, first_id)):\n",
    "                    cv2.rectangle(frame, (int(bboxes_xyxy[0]), int(bboxes_xyxy[1])), (int(bboxes_xyxy[2]), int(bboxes_xyxy[3])), (0,0,255), 2)\n",
    "                    cv2.putText(frame, f\"Pedestrian-{first_id}\", (int(bboxes_xyxy[0]) + 10, int(bboxes_xyxy[1]) - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0,0,255), 1) \n",
    "                \n",
    "                frame_bgr = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
    "                frame_bgr = cv2.resize(frame_bgr, (width, height))\n",
    "            else:\n",
    "                frame = cv2.resize(frame, (width, height))\n",
    "                frame_bgr = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)                \n",
    "                '''\n",
    "        #display_image(frame_bgr)\n",
    "        output_video.write(frame_bgr)\n",
    "    #print(\"FRAME\")\n",
    "# Liberar recursos\n",
    "output_video.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YOLOv8 Track all objects in a class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLO(model_path)\n",
    "\n",
    "objects = ['motor']\n",
    "number_class_list = []\n",
    "if objects!=None:\n",
    "    if objects!=['all']:\n",
    "        for object in objects: \n",
    "            number_class_list.append(classes.index(object))\n",
    "    elif objects == ['all']:\n",
    "        number_class_list = list(range(len(classes)))\n",
    "\n",
    "# Images folder path\n",
    "#img_folder = os.path.join(sequences_folder, 'uav0000024_00000_s') #people\n",
    "img_folder = os.path.join(sequences_folder, 'uav0000029_01102_s') #motor\n",
    "\n",
    "#img_folder = os.path.join(dataset_path, r'sequences\\uav0000120_04775_v')\n",
    "#img_folder = os.path.join(dataset_path, r'sequences\\uav0000370_00001_v')\n",
    "img_names = sorted(os.listdir(img_folder))\n",
    "\n",
    "# Video size\n",
    "width, height = 1280, 720\n",
    "\n",
    "# Codec and VideoWriter\n",
    "codec = cv2.VideoWriter_fourcc(*'XVID')\n",
    "fps = 30\n",
    "output_video = cv2.VideoWriter(r'.\\outputs\\Task3\\output_yolo.mp4', codec, fps, (width, height))\n",
    "\n",
    "# Frame processing\n",
    "for i, img in enumerate(img_names):\n",
    "    img_fullpath = os.path.join(img_folder, img)\n",
    "    frame = cv2.imread(img_fullpath)\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    if frame is not None:\n",
    "        # Redimensionar la imagen si es necesario\n",
    "        if width and height:\n",
    "            results = model.track(frame, persist=True, verbose=False, conf=0.3, iou=0.5, classes=number_class_list, imgsz=704)\n",
    "            annotated_frame = results[0].plot()               \n",
    "            #display_image(annotated_frame)\n",
    "            annotated_frame = cv2.resize(annotated_frame, (width, height))\n",
    "            output_video.write(cv2.cvtColor(annotated_frame, cv2.COLOR_RGB2BGR))\n",
    "    #print(\"FRAME\")\n",
    "# Liberar recursos\n",
    "output_video.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep-Sort for object tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deep_sort.utils.parser import get_config\n",
    "from deep_sort.deep_sort import DeepSort\n",
    "from deep_sort.sort.tracker import Tracker\n",
    "\n",
    "deep_sort_weights = 'deep_sort/deep/checkpoint/ckpt.t7'\n",
    "tracker = DeepSort(model_path=deep_sort_weights, max_age=70) #max_age=max number of frame until the model discart the prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Images folder path\n",
    "img_folder = os.path.join(sequences_folder, 'uav0000024_00000_s')\n",
    "#img_folder = os.path.join(dataset_path, r'sequences\\uav0000120_04775_v')\n",
    "#img_folder = os.path.join(dataset_path, r'sequences\\uav0000370_00001_v')\n",
    "img_names = sorted(os.listdir(img_folder))\n",
    "\n",
    "# Video size\n",
    "width, height = 1280, 720\n",
    "\n",
    "# Codec and VideoWriter\n",
    "codec = cv2.VideoWriter_fourcc(*'XVID')\n",
    "fps = 30\n",
    "output_video = cv2.VideoWriter(r'.\\outputs\\Task3\\output.mp4', codec, fps, (width, height))\n",
    "\n",
    "# Frame processing\n",
    "for img in img_names:\n",
    "    img_fullpath = os.path.join(img_folder, img)\n",
    "    frame = cv2.imread(img_fullpath)\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    if frame is not None:\n",
    "        # Redimensionar la imagen si es necesario\n",
    "        if width and height:\n",
    "            result_boxes = detector.detect(frame, objects=['pedestrian'], conf_thresh=0.6, imgsz=640)\n",
    "            # Detection control. If no detections were found, use the original frame\n",
    "            if len(result_boxes[0].cls) > 0:\n",
    "                pred_cls = result_boxes[0].cls.detach().cpu().numpy()\n",
    "                conf = result_boxes[0].conf.detach().cpu().numpy()\n",
    "                xyxy = result_boxes[0].xyxy.detach().cpu().numpy()\n",
    "                bboxes_xywh = result_boxes[0].xywh.detach().cpu().numpy()\n",
    "                #print(pred_cls)\n",
    "                \n",
    "                tracks = tracker.update(bboxes_xywh, conf, frame)\n",
    "                for track in tracker.tracker.tracks:\n",
    "                    track_id = track.track_id\n",
    "                    x1, y1, x2, y2 = track.to_tlbr()  # Get bounding box coordinates in (x1, y1, x2, y2) format\n",
    "                    w = x2 - x1  # Calculate width\n",
    "                    h = y2 - y1  # Calculate height\n",
    "\n",
    "                    #print(pred_cls, i)\n",
    "                    #print(classes[int(pred_cls[i])])\n",
    "                    cv2.rectangle(frame, (int(x1), int(y1)), (int(x1 + w), int(y1 + h)), (0,0,255), 2)\n",
    "                    cv2.putText(frame, f\"Pedestrian-{track_id}\", (int(x1) + 10, int(y1) - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0,0,255), 1)\n",
    "                    frame_bgr = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
    "                    frame_bgr = cv2.resize(frame_bgr, (width, height))\n",
    "                    \n",
    "            else:\n",
    "                frame = cv2.resize(frame, (width, height))\n",
    "                frame_bgr = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)                \n",
    "\n",
    "        output_video.write(frame_bgr)\n",
    "    #print(\"FRAME\")\n",
    "# Liberar recursos\n",
    "output_video.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.0 ('proyectos')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9046291224b9ae991683ef7a0ef8fd462791dccad48d624cd221e38879383f70"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
