{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import ultralytics\n",
    "from ultralytics import YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get OpenCV Perspective Transform (BEV) \n",
    "class ViewTransformer:\n",
    "    def __init__(self, source: np.ndarray, target: np.ndarray) -> None:\n",
    "        source = source.astype(np.float32)\n",
    "        target = target.astype(np.float32)\n",
    "        self.m = cv2.getPerspectiveTransform(source, target)\n",
    "\n",
    "    def transform_points(self, points: np.ndarray) -> np.ndarray:\n",
    "        if points.size == 0: #No detections\n",
    "            return points\n",
    "\n",
    "        reshaped_points = points.reshape(-1, 1, 2).astype(np.float32)\n",
    "        transformed_points = cv2.perspectiveTransform(reshaped_points, self.m)\n",
    "        return transformed_points.reshape(-1, 2)\n",
    "\n",
    "#Calculate the speed of each detected vehicle\n",
    "def get_speed(points_vector, frame_time):\n",
    "    global previous_y, first_loop\n",
    "\n",
    "    if first_loop:\n",
    "        previous_y = np.copy(points_vector)\n",
    "        first_loop = False\n",
    "        \n",
    "    if len(previous_y) <= len(points_vector):\n",
    "        previous_y_expanded = np.copy(points_vector)\n",
    "        previous_y_expanded[:len(previous_y)] = previous_y\n",
    "    else:\n",
    "        previous_y_expanded = np.copy(previous_y)\n",
    "        previous_y_expanded = previous_y_expanded[-len(points_vector):]\n",
    "        \n",
    "    #print(points_vector, previous_y_expanded)\n",
    "    #print(abs(points_vector - previous_y_expanded))\n",
    "        \n",
    "    previous_y = np.append(previous_y, points_vector)\n",
    "    previous_y = previous_y[-len(points_vector):]\n",
    "    distance_vector = abs(points_vector - previous_y_expanded)\n",
    "    speed_vector = (distance_vector / frame_time) * 3.6# speed = distance / time [m/s] * 3.6 = [km/h]\n",
    "    \n",
    "    return speed_vector\n",
    "\n",
    "#Determine if a point is inside a polygon\n",
    "def point_in_polygon(x, y, polygon):\n",
    "    result = cv2.pointPolygonTest(polygon, (x, y), False)\n",
    "    return result >= 0\n",
    "\n",
    "#Draw text with bakground on image\n",
    "def draw_text(img, text,\n",
    "          font=cv2.FONT_HERSHEY_PLAIN,\n",
    "          pos=(0, 0),\n",
    "          font_scale=3,\n",
    "          font_thickness=2,\n",
    "          text_color=(0, 255, 0),\n",
    "          text_color_bg=(0, 0, 0)\n",
    "          ):\n",
    "\n",
    "    x, y = pos\n",
    "    text_size, _ = cv2.getTextSize(text, font, font_scale, font_thickness)\n",
    "    text_w, text_h = text_size\n",
    "    cv2.rectangle(img, pos, (x + text_w, y + text_h), text_color_bg, -1)\n",
    "    cv2.putText(img, text, (int(x), int(y + text_h + font_scale - 1)), font, font_scale, text_color, font_thickness)\n",
    "\n",
    "    return text_size           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In Praga, where the video comes from, the lenght of the white line across the road is about 3m and 6m among they,\n",
    "#so the polygon shaped by the vertexes may cover a total distance of 65m x 25m aproximately.\n",
    "#Only for windows_view purpose we'll define view_transformer_for_window using vertexes * 10, because with 65m x 25m the windows would be too small\n",
    "\n",
    "YOLO_model_path = r'.\\models\\yolov8m.pt'\n",
    "video_path = 'road.mp4'\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "codec = cv2.VideoWriter_fourcc(*'XVID')\n",
    "width = cap.get(cv2.CAP_PROP_FRAME_WIDTH)\n",
    "height = cap.get(cv2.CAP_PROP_FRAME_HEIGHT)\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "time_among_frames = 1/fps\n",
    "\n",
    "vertexes = np.array([[587, 391], [818, 391], [1395, 719], [-50, 719]]) #Points that define the inference zone\n",
    "vertexes = vertexes.reshape((-1, 1, 2))\n",
    "\n",
    "TARGET_WIDTH = 25 \n",
    "TARGET_HEIGHT = 65\n",
    "target_vertexes = np.array([[0, 0], [TARGET_WIDTH - 1, 0], [TARGET_WIDTH - 1, TARGET_HEIGHT - 1], [0, TARGET_HEIGHT - 1]])\n",
    "target_vertexes = target_vertexes.reshape((-1, 1, 2))\n",
    "view_transformer = ViewTransformer(vertexes, target_vertexes)\n",
    "\n",
    "#Only for view purposes\n",
    "TARGET_WIDTH_for_view = 25 * 10 \n",
    "TARGET_HEIGHT_for_view = 65 * 10\n",
    "target_vertexes_for_view = np.array([[0, 0], [TARGET_WIDTH_for_view - 1, 0], [TARGET_WIDTH_for_view - 1, TARGET_HEIGHT_for_view - 1], [0, TARGET_HEIGHT_for_view - 1]])\n",
    "target_vertexes_for_view = target_vertexes_for_view.reshape((-1, 1, 2))\n",
    "view_transformer_for_window = ViewTransformer(vertexes, target_vertexes_for_view)\n",
    "\n",
    "#output_video = cv2.VideoWriter(r'.\\outputs\\detected.mp4', codec, fps, (width, height)) #Save output video\n",
    "model = YOLO(YOLO_model_path)\n",
    "\n",
    "first_loop = True\n",
    "previous_y = []\n",
    "display_option = 'Annotated_original' #'Original','Annotated_original','BEV'\n",
    "\n",
    "#Frame processing\n",
    "# Loop until the end of the video\n",
    "while (cap.isOpened()):\n",
    "    \n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if frame is not None:\n",
    "        results = model.track(frame, show=False, persist=True, verbose=False, conf=0.3, iou=0.5, classes=[0,1,2,3,5,7], imgsz=736, tracker=\"botsort.yaml\") #tracker=\"bytetrack.yaml\"\n",
    "        frame_cp = np.copy(frame)\n",
    "\n",
    "        for result in results:\n",
    "            boxes = result.boxes.xywh.cpu().numpy()\n",
    "            ids = result.boxes.id.cpu().numpy()       \n",
    "            poi_list = [[box[0], box[1] + box[3] / 2, int(id)] for box, id in zip(boxes, ids) if point_in_polygon(box[0], box[1] + box[3] / 2, vertexes)] #Use center down point (x, y, id)\n",
    "\n",
    "            if poi_list:\n",
    "                id_poi = [poi[-1] for poi in poi_list]\n",
    "                coord_poi = [poi[:2] for poi in poi_list]\n",
    "                transformed_poi = view_transformer.transform_points(np.array(coord_poi)) #Trasnform point from image coordinate to real coordinates\n",
    "                y_transformed = transformed_poi[:,1] #Get only the y coordinate transformed for each ID\n",
    "                \n",
    "                speed_vector = get_speed(y_transformed, time_among_frames)\n",
    "                speed_coord_id = zip(speed_vector, coord_poi, id_poi)\n",
    "\n",
    "                for speed, coord, id in speed_coord_id:\n",
    "                    draw_text(frame_cp, text=f'ID:{id} - {int(speed)} km/h', font=cv2.FONT_HERSHEY_SIMPLEX, pos=(int(coord[0]), int(coord[1]+5)),\n",
    "                              font_scale=0.5, font_thickness=1, text_color=(0,0,0), text_color_bg=(0,255,255)) \n",
    "\n",
    "        #Display Options\n",
    "        if display_option == 'BEV':\n",
    "            if poi_list:\n",
    "                # Display the annotated frame with points on each detected vehicle\n",
    "                transformed_poi_window = view_transformer_for_window.transform_points(np.array([coord_poi])) #Trasnform point from image coordinate to real coordinates multiply by 10\n",
    "                imgOutput = cv2.warpPerspective(frame, view_transformer_for_window.m, (TARGET_WIDTH_for_view, TARGET_HEIGHT_for_view), cv2.INTER_LINEAR, borderMode=cv2.BORDER_CONSTANT, borderValue=(0,0,0))\n",
    "                for xy_point in transformed_poi_window:\n",
    "                    cv2.circle(imgOutput, (int(xy_point[0]),int(xy_point[1])), radius=4, color=(255,0,0), thickness=-1)\n",
    "                cv2.imshow(\"YOLOv8 Tracking_BEV\", imgOutput)\n",
    "            else:\n",
    "                transformed_poi_window = view_transformer_for_window.transform_points(np.array([coord_poi])) #Trasnform point from image coordinate to real coordinates multiply by 10\n",
    "                imgOutput = cv2.warpPerspective(frame, view_transformer_for_window.m, (TARGET_WIDTH_for_view, TARGET_HEIGHT_for_view), cv2.INTER_LINEAR, borderMode=cv2.BORDER_CONSTANT, borderValue=(0,0,0))\n",
    "                cv2.imshow(\"YOLOv8 Tracking_BEV\", imgOutput)\n",
    "        \n",
    "        elif display_option == 'Annotated_original':\n",
    "            # Display the annotated frame with points\n",
    "            for xy_point in poi_list:\n",
    "                cv2.circle(frame_cp, (int(xy_point[0]),int(xy_point[1])), radius=2, color=(255,0,0), thickness=-1)\n",
    "            frame_cp = cv2.polylines(frame_cp, [vertexes], isClosed=True, color=(0, 0, 255), thickness=1)\n",
    "            cv2.imshow(\"YOLOv8 Tracking\", frame_cp)\n",
    "        \n",
    "        elif display_option == 'Original':\n",
    "            cv2.imshow(\"YOLOv8 Tracking\", frame)             \n",
    "\n",
    "    # define q as the exit button\n",
    "    if cv2.waitKey(25) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# release the video capture object\n",
    "cap.release()\n",
    "# Closes all the windows currently opened.\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.0 ('proyectos')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9046291224b9ae991683ef7a0ef8fd462791dccad48d624cd221e38879383f70"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
